{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "MIN_SUPPORT = 0.018\n",
    "MIN_CONFIDENT = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nData Ingestion:\\n3898 samples, 167 products\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('./data/apriori_data.csv')\n",
    "'''\n",
    "Data Ingestion:\n",
    "3898 samples, 167 products\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(raw_data):\n",
    "    transactions = []\n",
    "    for i in raw_data.values:\n",
    "        transactions.append(list(*np.where(i==1)))              \n",
    "    return transactions\n",
    "\n",
    "def support_count(transactions, itemsets):\n",
    "    candidate_set = {}\n",
    "    total_transactions = len(transactions)\n",
    "    for itemset in itemsets:    \n",
    "        sup_count = np.sum([1 for transaction in transactions if itemset.issubset(transaction)])\n",
    "        candidate_set[itemset] = sup_count / total_transactions\n",
    "    return candidate_set\n",
    "\n",
    "def filter_candidates(candidate_set, min_support):\n",
    "    filtered_set = {}\n",
    "    for itemset, support in candidate_set.items():\n",
    "        if support >= min_support:\n",
    "            filtered_set[itemset] = support\n",
    "    return filtered_set\n",
    "\n",
    "def generate_candidates_itemsets(filter_set, k_step):\n",
    "    candidate_itemsets = set()\n",
    "    freq_items = list(filter_set.keys())    \n",
    "    \n",
    "    for i in range(len(freq_items)):\n",
    "        for j in range(i+1, len(freq_items)):\n",
    "            itemset = freq_items[i].union(freq_items[j])           \n",
    "            if len(itemset)==k_step:\n",
    "                candidate_itemsets.add(itemset)\n",
    "    \n",
    "    return candidate_itemsets\n",
    "\n",
    "def apriori(transactions, min_support, preprocess = True):\n",
    "    # Initial frequent 1-itemsets\n",
    "    if preprocess:\n",
    "        transactions = preprocess_data(transactions)\n",
    "    \n",
    "    itemsets_c1 = set(frozenset([item]) for transaction in transactions for item in transaction)\n",
    "    candidate_set = support_count(transactions, itemsets_c1)\n",
    "    k_step = 2\n",
    "\n",
    "    filter_set = filter_candidates(candidate_set, min_support=min_support)\n",
    "    final_set = filter_set.copy()\n",
    "    \n",
    "    while filter_set:\n",
    "        candidate_itemset = generate_candidates_itemsets(filter_set, k_step)\n",
    "        candidate_set = support_count(transactions, candidate_itemset)\n",
    "        filter_set = filter_candidates(candidate_set, min_support=min_support)\n",
    "        final_set.update(filter_set)\n",
    "        k_step+=1           \n",
    "    \n",
    "    return final_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_association = apriori(transactions=raw_data, min_support=MIN_SUPPORT)\n",
    "\n",
    "association = []\n",
    "for key in final_association.keys():\n",
    "    if len(key) == 3:\n",
    "        association.append(key)\n",
    "\n",
    "result_dict = {}\n",
    "for itemset in association:\n",
    "# Generate all possible antecedent and consequent pairs\n",
    "    for i in range(1, len(itemset)):  # Split itemset into antecedent of size 1, 2\n",
    "        for antecedent in combinations(itemset, i):\n",
    "            antecedent = frozenset(antecedent)\n",
    "            consequent = itemset - antecedent  # Remainder of the itemset as consequent\n",
    "\n",
    "            # Calculate confidence\n",
    "            confidence = final_association.get(itemset, 0) / final_association.get(antecedent, 1)\n",
    "\n",
    "            # Check if confidence meets min_confidence threshold\n",
    "            if confidence >= MIN_CONFIDENT:\n",
    "                # conf = round(confidence,2)       \n",
    "                if confidence not in result_dict:\n",
    "                    result_dict[confidence] = [itemset] \n",
    "                else:\n",
    "                    result_dict[confidence].append(itemset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = []\n",
    "\n",
    "#Generate result file\n",
    "for conf in result_dict.keys():\n",
    "    row = ''\n",
    "    for item in result_dict[conf]:\n",
    "        item_list = list(item)\n",
    "        row = row + '||'\n",
    "        for i in range(len(item_list)):\n",
    "            row = row + '(' + 'PRODUCT' + str(item_list[i]) + ')' + '^'\n",
    "    \n",
    "    result_data.append([row[2:-1], conf])\n",
    "    \n",
    "result_df = pd.DataFrame(result_data)\n",
    "result_df.to_csv('./results/result4.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
