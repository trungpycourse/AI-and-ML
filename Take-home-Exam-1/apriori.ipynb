{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "MIN_SUPPORT = 0.018\n",
    "MIN_CONFIDENT = 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nData Ingestion:\\n3898 samples, 167 products\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('./data/apriori_data.csv')\n",
    "'''\n",
    "Data Ingestion:\n",
    "3898 samples, 167 products\n",
    "'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(raw_data):\n",
    "    transactions = []\n",
    "    for i in raw_data.values:\n",
    "        transactions.append(list(*np.where(i==1)))              \n",
    "    return transactions\n",
    "\n",
    "def support_count(transactions, itemsets):\n",
    "    candidate_set = {}\n",
    "    total_transactions = len(transactions)\n",
    "    for itemset in itemsets:    \n",
    "        sup_count = np.sum([1 for transaction in transactions if itemset.issubset(transaction)])\n",
    "        candidate_set[itemset] = sup_count / total_transactions\n",
    "    return candidate_set\n",
    "\n",
    "def filter_candidates(candidate_set, min_support):\n",
    "    filtered_set = {}\n",
    "    for itemset, support in candidate_set.items():\n",
    "        if support >= min_support:\n",
    "            filtered_set[itemset] = support\n",
    "    return filtered_set\n",
    "\n",
    "def generate_candidates_itemsets(filter_set, k_step):\n",
    "    candidate_itemsets = set()\n",
    "    freq_items = list(filter_set.keys())    \n",
    "    \n",
    "    for i in range(len(freq_items)):\n",
    "        for j in range(i+1, len(freq_items)):\n",
    "            itemset = freq_items[i].union(freq_items[j])           \n",
    "            if len(itemset)==k_step:\n",
    "                candidate_itemsets.add(itemset)\n",
    "    \n",
    "    return candidate_itemsets\n",
    "\n",
    "def apriori(transactions, min_support, preprocess = True):\n",
    "    # Initial frequent 1-itemsets\n",
    "    if preprocess:\n",
    "        transactions = preprocess_data(transactions)\n",
    "    \n",
    "    itemsets_c1 = set(frozenset([item]) for transaction in transactions for item in transaction)\n",
    "    candidate_set = support_count(transactions, itemsets_c1)\n",
    "    k_step = 2\n",
    "\n",
    "    filter_set = filter_candidates(candidate_set, min_support=min_support)\n",
    "    final_set = filter_set.copy()\n",
    "    \n",
    "    while filter_set:\n",
    "        candidate_itemset = generate_candidates_itemsets(filter_set, k_step)\n",
    "        candidate_set = support_count(transactions, candidate_itemset)\n",
    "        filter_set = filter_candidates(candidate_set, min_support=min_support)\n",
    "        final_set.update(filter_set)\n",
    "        k_step+=1        \n",
    "        \n",
    "    \n",
    "    return final_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# transactions_test = [[1,2,5], [2,4], [2,3], [1,2,4], [1,3], [2,3], [1,3], [1,2,3,5], [1,2,3]]\n",
    "# final = apriori(transactions=transactions_test, min_support=MIN_SUPPORT, preprocess=False)\n",
    "# print(f\"Final support: {final}\")\n",
    "\n",
    "# association = []\n",
    "# for key in final.keys():\n",
    "#     if len(key) == 3:\n",
    "#         association.append(key)\n",
    "# print(f\"Asociation: {association}\")\n",
    "\n",
    "# MIN_SUPPORT = 2/9\n",
    "# MIN_CONFIDENT = 0.5\n",
    "\n",
    "# for itemset in association:\n",
    "# # Generate all possible antecedent and consequent pairs\n",
    "#     for i in range(1, len(itemset)):  # Split itemset into antecedent of size 1, 2\n",
    "#         for antecedent in combinations(itemset, i):\n",
    "#             antecedent = frozenset(antecedent)\n",
    "#             consequent = itemset - antecedent  # Remainder of the itemset as consequent\n",
    "            \n",
    "#             # print(f'antecedent: {antecedent}')\n",
    "#             # print(f'consequent: {consequent}')\n",
    "#             # Calculate confidence\n",
    "#             confidence = final.get(itemset, 0) / final.get(antecedent, 1)\n",
    "\n",
    "#             # Check if confidence meets min_confidence threshold\n",
    "#             if (round(confidence,1)) >= MIN_CONFIDENT:\n",
    "#                 conf = round(confidence,2)\n",
    "#                 print('-'*20)\n",
    "#                 print(f\"Rule {antecedent} => {consequent} is valid with confidence {confidence:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_association = apriori(transactions=raw_data, min_support=MIN_SUPPORT)\n",
    "\n",
    "association = []\n",
    "for key in final_association.keys():\n",
    "    if len(key) == 3:\n",
    "        association.append(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.6148648648648649: [frozenset({12, 14, 164})],\n",
       " 0.6178861788617886: [frozenset({49, 130, 164})],\n",
       " 0.6158536585365855: [frozenset({14, 164, 165})],\n",
       " 0.62: [frozenset({28, 102, 164})],\n",
       " 0.6133333333333333: [frozenset({8, 164, 165})],\n",
       " 0.6061776061776062: [frozenset({12, 164, 165})],\n",
       " 0.60625: [frozenset({63, 122, 164})],\n",
       " 0.602803738317757: [frozenset({133, 164, 165})],\n",
       " 0.6564885496183206: [frozenset({20, 133, 164})],\n",
       " 0.6046511627906976: [frozenset({8, 123, 164}), frozenset({12, 40, 164})],\n",
       " 0.6305732484076433: [frozenset({40, 164, 165})],\n",
       " 0.6159420289855073: [frozenset({68, 102, 164})],\n",
       " 0.6027397260273972: [frozenset({28, 122, 164})],\n",
       " 0.6756756756756757: [frozenset({49, 105, 164})],\n",
       " 0.613095238095238: [frozenset({40, 138, 164})],\n",
       " 0.6007462686567164: [frozenset({122, 133, 164})],\n",
       " 0.6012658227848101: [frozenset({8, 122, 164})],\n",
       " 0.6240000000000001: [frozenset({28, 164, 165})],\n",
       " 0.601010101010101: [frozenset({8, 102, 164})],\n",
       " 0.6324786324786325: [frozenset({95, 133, 164})],\n",
       " 0.6228070175438596: [frozenset({11, 160, 164})],\n",
       " 0.6: [frozenset({105, 109, 164}), frozenset({15, 164, 165})],\n",
       " 0.6610169491525424: [frozenset({11, 133, 164})],\n",
       " 0.6056910569105692: [frozenset({11, 122, 164})],\n",
       " 0.628099173553719: [frozenset({138, 162, 164})],\n",
       " 0.6454545454545454: [frozenset({90, 102, 164})],\n",
       " 0.6017699115044248: [frozenset({105, 164, 165})],\n",
       " 0.6030534351145038: [frozenset({11, 105, 164})],\n",
       " 0.6162162162162161: [frozenset({123, 133, 164})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict = {}\n",
    "for itemset in association:\n",
    "# Generate all possible antecedent and consequent pairs\n",
    "    for i in range(1, len(itemset)):  # Split itemset into antecedent of size 1, 2\n",
    "        for antecedent in combinations(itemset, i):\n",
    "            antecedent = frozenset(antecedent)\n",
    "            consequent = itemset - antecedent  # Remainder of the itemset as consequent\n",
    "\n",
    "            # Calculate confidence\n",
    "            confidence = final_association.get(itemset, 0) / final_association.get(antecedent, 1)\n",
    "\n",
    "            # Check if confidence meets min_confidence threshold\n",
    "            if confidence >= MIN_CONFIDENT:\n",
    "                # conf = round(confidence,2)       \n",
    "                if confidence not in result_dict:\n",
    "                    result_dict[confidence] = [itemset] \n",
    "                else:\n",
    "                    result_dict[confidence].append(itemset)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with itemset [frozenset({164, 12, 14})]: and type <class 'list'>\n",
      "with itemset [frozenset({49, 130, 164})]: and type <class 'list'>\n",
      "with itemset [frozenset({164, 165, 14})]: and type <class 'list'>\n",
      "with itemset [frozenset({164, 28, 102})]: and type <class 'list'>\n",
      "with itemset [frozenset({8, 164, 165})]: and type <class 'list'>\n",
      "with itemset [frozenset({164, 12, 165})]: and type <class 'list'>\n",
      "with itemset [frozenset({122, 164, 63})]: and type <class 'list'>\n",
      "with itemset [frozenset({164, 133, 165})]: and type <class 'list'>\n",
      "with itemset [frozenset({20, 164, 133})]: and type <class 'list'>\n",
      "with itemset [frozenset({8, 123, 164}), frozenset({40, 164, 12})]: and type <class 'list'>\n",
      "with itemset [frozenset({40, 164, 165})]: and type <class 'list'>\n",
      "with itemset [frozenset({164, 68, 102})]: and type <class 'list'>\n",
      "with itemset [frozenset({164, 28, 122})]: and type <class 'list'>\n",
      "with itemset [frozenset({105, 164, 49})]: and type <class 'list'>\n",
      "with itemset [frozenset({40, 138, 164})]: and type <class 'list'>\n",
      "with itemset [frozenset({122, 164, 133})]: and type <class 'list'>\n",
      "with itemset [frozenset({8, 122, 164})]: and type <class 'list'>\n",
      "with itemset [frozenset({164, 28, 165})]: and type <class 'list'>\n",
      "with itemset [frozenset({8, 164, 102})]: and type <class 'list'>\n",
      "with itemset [frozenset({164, 133, 95})]: and type <class 'list'>\n",
      "with itemset [frozenset({160, 11, 164})]: and type <class 'list'>\n",
      "with itemset [frozenset({105, 164, 109}), frozenset({164, 165, 15})]: and type <class 'list'>\n",
      "with itemset [frozenset({11, 164, 133})]: and type <class 'list'>\n",
      "with itemset [frozenset({122, 11, 164})]: and type <class 'list'>\n",
      "with itemset [frozenset({162, 164, 138})]: and type <class 'list'>\n",
      "with itemset [frozenset({90, 164, 102})]: and type <class 'list'>\n",
      "with itemset [frozenset({105, 164, 165})]: and type <class 'list'>\n",
      "with itemset [frozenset({105, 11, 164})]: and type <class 'list'>\n",
      "with itemset [frozenset({123, 164, 133})]: and type <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "result_data = []\n",
    "\n",
    "#Generate result file\n",
    "for conf in result_dict.keys():\n",
    "    row = ''\n",
    "    for item in result_dict[conf]:\n",
    "        item_list = list(item)\n",
    "        row = row + '||'\n",
    "        for i in range(len(item_list)):\n",
    "            row = row + '(' + 'PRODUCT' + str(item_list[i]) + ')' + '^'\n",
    "    \n",
    "    result_data.append([row[2:-1], conf])\n",
    "    \n",
    "result_df = pd.DataFrame(result_data)\n",
    "result_df.to_csv('./results/result4.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
